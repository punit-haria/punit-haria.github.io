<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>A Multimodal Example using Images | PS Musing</title>

<meta property='og:title' content='A Multimodal Example using Images - PS Musing'>
<meta property='og:description' content='Previously, I described the general multimodal learning problem, outlined semi-supervised and unsupervised versions of the problem, and how one could approach them using deep generative models. The focus here is on illustrating these ideas with an example.
Colored MNIST The toy dataset we use in our experiments is built by taking deterministic transforms of MNIST to craft two separate modalities. For the first group, we paint each digit randomly using one of three colours, and for the second group, we apply a Sobel filter to extract an edge map of each digit and then paint each digit using one of three colours.'>
<meta property='og:url' content='https://punit-haria.github.io/posts/multimodal_example/'>
<meta property='og:site_name' content='PS Musing'>
<meta property='og:type' content='article'><meta property='og:image' content='https://www.gravatar.com/avatar/76d14f306d44ac5c6b2d010fef6889ce?s=256'><meta property='article:section' content='Posts'><meta property='article:published_time' content='2018-05-15T00:00:00Z'><meta property='article:modified_time' content='2018-05-15T00:00:00Z'><meta name='twitter:card' content='summary'><meta name='twitter:site' content='@sh_punit'><meta name='twitter:creator' content='@sh_punit'>


<link href="https://punit-haria.github.io/index.xml" rel="alternate" type="application/rss+xml" title="PS Musing">

<link rel="stylesheet" href="/css/style.css"><link rel='stylesheet' href='https://punit-haria.github.io/css/custom.css'><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<link rel="canonical" href="https://punit-haria.github.io/posts/multimodal_example/">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>

<section class="section">
  <div class="container">
    <nav id="nav-main" class="nav">
      <div id="nav-name" class="nav-left">
        <a id="nav-anchor" class="nav-item" href="https://punit-haria.github.io/">
          <h1 id="nav-heading" class="title is-4">PS Musing</h1>
        </a>
      </div>
      <div class="nav-right">
        <nav id="nav-items" class="nav-item level is-mobile"><a class="level-item" aria-label="github" href='https://github.com/punit-haria'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="twitter" href='https://twitter.com/sh_punit'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="email" href='mailto:shahpunit@google.com'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
    <polyline points="22,6 12,13 2,6"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="linkedin" href='https://linkedin.com/in/punitharia'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path stroke-width="1.8" d="m5.839218,4.101561c0,1.211972 -0.974141,2.194011 -2.176459,2.194011s-2.176459,-0.982039 -2.176459,-2.194011c0,-1.211094 0.974141,-2.194011 2.176459,-2.194011s2.176459,0.982917 2.176459,2.194011zm0.017552,3.94922l-4.388022,0l0,14.04167l4.388022,0l0,-14.04167zm7.005038,0l-4.359939,0l0,14.04167l4.360816,0l0,-7.370999c0,-4.098413 5.291077,-4.433657 5.291077,0l0,7.370999l4.377491,0l0,-8.89101c0,-6.915523 -7.829986,-6.66365 -9.669445,-3.259423l0,-1.891237z"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>

    <nav class="nav">
      

      
    </nav>

  </div>
  <script src="/js/navicon-shift.js"></script>
</section>
<section class="section">
  <div class="container">
    <div class="subtitle tags is-6 is-pulled-right">
      
    </div>
    <h2 class="subtitle is-6">May 15, 2018</h2>
    <h1 class="title">A Multimodal Example using Images</h1>
    
    <div class="content">
      <p><a href="/posts/multimodal/">Previously</a>, I described the general multimodal learning problem, outlined semi-supervised and unsupervised versions of the problem, and how one could approach them using deep generative models. The focus here is on illustrating these ideas with an example.</p>
<h3 id="colored-mnist">Colored MNIST</h3>
<p>The toy dataset we use in our experiments is built by taking deterministic transforms of MNIST to craft two separate modalities. For the first group, we paint each digit randomly using one of three colours, and for the second group, we apply a <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.sobel.html">Sobel filter</a> to extract an edge map of each digit and then paint each digit using one of three colours. Importantly, there is a one-to-one mapping between the colours in each group. The creation of this dataset is based on <a href="https://arxiv.org/abs/1703.00848">Liu et al. (2017)</a></p>
<figure class="center">
	<img src="/img/multimodal/color_test1.png" style="float: left; width:30%; margin-left: 10%; margin-right: 20%">
	<img src="/img/multimodal/color_test2.png" style="float: left; width:30%; margin-right: 10%">
	<p style="clear: both;"></p>
    <figcaption>
		  <h6>Test samples from the coloured MNIST dataset: modality one on the left, and modality two on the right. Blue is mapped to blue, green to green, and red to yellow.</h6>
    </figcaption>
</figure>

<p>We can split the data so that we have one subset of images from each group that are <em>paired</em> together, and we can have two larger sets of images from each group that remain unpaired. This simulates a very realistic scenario in the wild of having a few paired examples and a much greater collection of unpaired data.</p>
<h3 id="multimodal-variational-autoencoder">Multimodal Variational Autoencoder</h3>
<p>Consider a generative process which begins by sampling a latent $\mathbf{x}$ from a prior distribution, and subsequently samples two output modalities $\mathbf{y}_1$ and $\mathbf{y}_2$ from a conditional distribution $p_{\theta}(\mathbf{y}_1, \mathbf{y}_2 \vert \mathbf{x}) = p_{\theta}(\mathbf{y}_1 \vert \mathbf{x} ) p_{\theta}(\mathbf{y}_2 \vert \mathbf{x} )$.</p>
<figure class="center">
    <img src="/img/multimodal/multimodal_vae.png" width="20%"/> 
</figure>

<p>Our MNIST-based observations are discrete, so we use a multivariate categorical distribution for the conditional likelihood:</p>

$$ p_{\theta}(\mathbf{y}_1 \vert \mathbf{x}) \ = \ \prod_{i=1}^m \prod_{j=0}^{255} \alpha_{ij}^{\mathbb{I}[y_1^{(i)} = j]}.$$

<p>Each image $\mathbf{y}_1$ is represented in vectorized form with dimension $m$. Typically, $m$ is the product of the image&rsquo;s spatial dimensions with the number of colour channels. The vector $\mathbf{x} \in \mathbb{R}^k$ is the latent variable, and the operator $\mathbb{I}[\cdot]$ is the indicator function. Each $\alpha_{ij}$ is the probability that $y_1^{(i)} = j$, and we have that $\sum_{j=0}^{255} \alpha_{ij} = 1$ for $i=1,\ldots,m$. Note that the set ${ \alpha_{ij}}$ are outputs of a deep neural network $f_{\theta, 1}(\mathbf{x})$ with parameters $\theta$. While this conditional likelihood is relatively simple, the marginal likelihood is much more complex and well-suited to modeling image data. Note that the same specifications and reasoning apply to the second modality $\mathbf{y}_2$.</p>
<p>The latent $\mathbf{x}$ can be interpretted as the explanatory factors of variation underlying the observed data, and it&rsquo;s quite natural to think of these factors as mutually independent. Therefore, we choose a simple prior $p(\mathbf{x}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ which can be factorized. The variational distribution is similarly Gaussian, $q_{\phi}(\mathbf{x} \vert \mathbf{y}_1, \mathbf{y}_2) = \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma}^2 \mathbf{I})$, but the vectors $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ are outputs of a deep neural network $f_{\phi}(\mathbf{y}_1, \mathbf{y}_2)$ with parameters $\phi$. Since we&rsquo;re working with image data, we can use a convolutional layers in our encoder network $f_{\phi}(\mathbf{y}_1, \mathbf{y}_2)$, and deconvolutional or upsampling layers in our decoder networks $f_{\theta, 1}(\mathbf{x})$ and $f_{\theta, 2}(\mathbf{x})$ (see <a href="https://github.com/punit-haria/multimodal-learning/blob/master/code/models/layers.py">implementation</a>).</p>
<p>We now have all the building blocks for inference and learning. As shown <a href="/posts/multimodal/">here</a> and <a href="/posts/vae/">here</a>, inference and learning amounts to an optimization problem which can be solved using stochastic gradient methods. We are to maximize the following:</p>

$$ \mathcal{F}_{\mathbf{y}_1, \mathbf{y}_2}(\theta, \phi) \ = \ \frac{1}{S} \sum_{s=1}^S \log \frac{p_{\theta}(\mathbf{x}^{(s)}, \mathbf{y}_1, \mathbf{y_2})}{q_{\phi}(\mathbf{x}^{(s)} \vert \mathbf{y}_1, \mathbf{y}_2)}$$

<p>where each $\mathbf{x}^{(s)} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}^{(s)}$ and each $\boldsymbol{\epsilon}^{(s)}$ is a sample from $\mathcal{N}(\mathbf{0}, \mathbf{I})$.</p>
<p>Note that for the missing data case, our multimodal approach regresses to a single modality variational autoencoder objective. Therefore, we are learning the variational and model parameters for the following distributions via the optimization process: $q_{\phi}(\mathbf{x} \vert \mathbf{y}_1, \mathbf{y}_2)$, $q_{\phi}(\mathbf{x}\vert \mathbf{y}_1)$, $q_{\phi}(\mathbf{x}\vert \mathbf{y}_2)$, $p_{\theta}(\mathbf{y}_1 \vert \mathbf{x})$, and $p_{\theta}(\mathbf{y}_2 \vert \mathbf{x})$. Notice that there are 3 variational distributions since we must learn to accomodate the cases with missing data. Hence, we now have everything we need to implement this model and learn shared representations of the coloured MNIST data.</p>
<p>Our approach allows us to generate synthetic versions of each modality as well as inferring one modality from the other. To demonstrate this latter case, suppose we have a new image $\mathbf{y}_1​$ from the first group. We can then infer its latent representation by sampling from the variational distribution $q_{\phi}(\mathbf{x} \vert \mathbf{y}_1)​$. Using this sampled $\mathbf{x}​$, we can then generate an image from the second group by sampling from $p_{\theta}(\mathbf{y}_2 \vert \mathbf{x})​$. The figure below depicts the results of such as inference procedure using the coloured MNIST dataset. A Tensorflow implementation of this model is available <a href="https://github.com/punit-haria/multimodal-learning/blob/master/code/models/jointvae.py">here</a>.</p>
<figure class="center">
	<img src="/img/multimodal/inference1.png" style="float: left; width:30%; margin-left: 10%; margin-right: 20%">
	<img src="/img/multimodal/inference2.png" style="float: left; width:30%; margin-right: 10%">
	<p style="clear: both;"></p>
    <figcaption>
		  <h6>Left: Edgemap reconstructions from solid digits using a convolutional variational autoencoder. Right: Solid digit reconstructions from edgemaps.</h6>
    </figcaption>
</figure>


      
      <div class="related">
</div>
      
    </div>
    
  </div>
</section>



<section class="section">
  <div class="container has-text-centered">
    <p>&copy; <a href="https://github.com/punit-haria">Punit Shah</a> 2017</p>
    
      <p>Powered by <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/ribice/kiss">Kiss</a>.</p>
    
  </div>
</section>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-187493056-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>





</body>
</html>

