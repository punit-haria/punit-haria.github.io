<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Textual Entailment | PS Musing</title>

<meta property='og:title' content='Textual Entailment - PS Musing'>
<meta property='og:description' content='Storytelling is inherent to our nature and is found in almost every human endeavor: speech, literature, film, theatre, music, the visual arts, and journalism, to name a few. At the earliest of ages, we are attuned to and even crave the experience of a good story, and crucially, they drive our development. They guide us, teach us about our histories and cultures, and shape us as we form our identities. In listening to and learning from these stories, we build on our innate ability to ground the words and symbols we perceive into the complex world around us.'>
<meta property='og:url' content='https://punit-haria.github.io/posts/textual_entailment/'>
<meta property='og:site_name' content='PS Musing'>
<meta property='og:type' content='article'><meta property='og:image' content='https://www.gravatar.com/avatar/76d14f306d44ac5c6b2d010fef6889ce?s=256'><meta property='article:section' content='Posts'><meta property='article:published_time' content='2017-12-08T00:00:00Z'><meta property='article:modified_time' content='2017-12-08T00:00:00Z'><meta name='twitter:card' content='summary'><meta name='twitter:site' content='@sh_punit'><meta name='twitter:creator' content='@sh_punit'>


<link href="https://punit-haria.github.io/index.xml" rel="alternate" type="application/rss+xml" title="PS Musing">

<link rel="stylesheet" href="/css/style.css"><link rel='stylesheet' href='https://punit-haria.github.io/css/custom.css'><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<link rel="canonical" href="https://punit-haria.github.io/posts/textual_entailment/">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>

<section class="section">
  <div class="container">
    <nav id="nav-main" class="nav">
      <div id="nav-name" class="nav-left">
        <a id="nav-anchor" class="nav-item" href="https://punit-haria.github.io/">
          <h1 id="nav-heading" class="title is-4">PS Musing</h1>
        </a>
      </div>
      <div class="nav-right">
        <nav id="nav-items" class="nav-item level is-mobile"><a class="level-item" aria-label="github" href='https://github.com/punit-haria'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="twitter" href='https://twitter.com/sh_punit'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="email" href='mailto:shahpunit@google.com'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
    <polyline points="22,6 12,13 2,6"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" aria-label="linkedin" href='https://linkedin.com/in/punitharia'
            target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path stroke-width="1.8" d="m5.839218,4.101561c0,1.211972 -0.974141,2.194011 -2.176459,2.194011s-2.176459,-0.982039 -2.176459,-2.194011c0,-1.211094 0.974141,-2.194011 2.176459,-2.194011s2.176459,0.982917 2.176459,2.194011zm0.017552,3.94922l-4.388022,0l0,14.04167l4.388022,0l0,-14.04167zm7.005038,0l-4.359939,0l0,14.04167l4.360816,0l0,-7.370999c0,-4.098413 5.291077,-4.433657 5.291077,0l0,7.370999l4.377491,0l0,-8.89101c0,-6.915523 -7.829986,-6.66365 -9.669445,-3.259423l0,-1.891237z"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>

    <nav class="nav">
      

      
    </nav>

  </div>
  <script src="/js/navicon-shift.js"></script>
</section>
<section class="section">
  <div class="container">
    <div class="subtitle tags is-6 is-pulled-right">
      
    </div>
    <h2 class="subtitle is-6">December 8, 2017</h2>
    <h1 class="title">Textual Entailment</h1>
    
    <div class="content">
      <p>Storytelling is inherent to our nature and is found in almost every human endeavor: speech, literature, film, theatre, music, the visual arts, and journalism, to name a few. At the earliest of ages, we are attuned to and even crave the experience of a good story, and crucially, they drive our development. They guide us, teach us about our histories and cultures, and shape us as we form our identities. In listening to and learning from these stories, we build on our innate ability to ground the words and symbols we perceive into the complex world around us. We also get introduced to the notions of logic and causality. We mostly take for granted our ability to make inferences and deductions, even though we do this countless times each day.</p>
<p>In natural language processing, endowing machines with this kind of commonsense reasoning is a really important research problem. Given two sentences, the problem of textual entailment is the problem of deciding whether the first sentence can be used to prove that the second is true, false, or totally unrelated. This ability is very useful in many applications including question-answering and text summarization. One interesting question we may ask is whether an agent can learn to make inferences and deductions from textual stories. This is of course an incredibly complex problem. Not only does the learning algorithm need to be able to interpret each word and sentence, but it needs to be able to correctly ground these words and their relationships in the real world.</p>
<h3 id="reordering-stories">Reordering Stories</h3>
<p>As a dramatic simplification of this problem, suppose we&rsquo;re given a short narrative which has been jumbled, that is, the sentences have been randomly shuffled. Can we build an algorithm to correctly reorder these sentences? To do this well, the algorithm will require both commonsense knowledge and temporal understanding. We can think of this as a prediction problem. For instance, a collection of input sentences could look like:</p>
<ul>
<li>She found a lamp she liked.</li>
<li>She went to the store.</li>
<li>She bought the lamp.</li>
<li>Joy decided to get a new lamp.</li>
<li>Joy&rsquo;s lamp broke.</li>
</ul>
<p>The learnt model needs to provide a solution of the form, 5 4 2 1 3, which corresponds to the following ordering of sentences:</p>
<ul>
<li>Joy&rsquo;s lamp broke.</li>
<li>Joy decided to get a new lamp.</li>
<li>She went to the store.</li>
<li>She found a lamp she liked.</li>
<li>She bought the lamp.</li>
</ul>
<p>Given about 50,000 examples like this one, the goal is to train a learner to predict the correct sequence permutation. Performance can be measured by counting the proportion of sentences placed in the correct position, so a perfect score would be $5/5 = 1$, and a random guess would get you a score of $0.2$ on average. There are many ways to approach the learning problem, and in this post, I&rsquo;ll describe one that uses recurrent neural networks. If you&rsquo;d like to know more about the dataset, you can find the corresponding paper <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/short-commonsense-stories.pdf">here</a>, and you can request download access <a href="http://cs.rochester.edu/nlp/rocstories/">here</a> (it&rsquo;s free to use).</p>
<hr>
<blockquote>
<p><strong>Aside</strong>
Calculating the random baseline expected value of $0.2$ is a combinatoric problem requiring the notion of <a href="https://en.wikipedia.org/wiki/Derangement">derangements</a>, which are permutations of a sequence such that none of the elements remain in their original place. The idea is to count all possible derangements of length 2, 3, 4, and 5 in order to compute the expected score for a random guess.</p>
</blockquote>
<hr>
<h3 id="word-embeddings">Word Embeddings</h3>
<p>We begin by creating <a href="/docs/refs/distributed_representations.pdf">distributed representations</a> of the words in our vocabulary to be used as input to the learner.  Distributed representations are at the heart of neural network philosophy, and we can analogize them to our own human memories, where events and facts are not stored in one local region, but across groupings of distributed cells and connections [1]. What this means for us is that instead of simply one-hot-encoding the words, we embed them into a real-valued $k$-dimensional vector space. In effect, each word is now represented by many elements of a vector, and each element in the vector is associated with many words. With this approach, each vector element is much more efficiently utilized and the neural network is now more resilient to inaccuracies or missing values (c.f. <a href="https://arxiv.org/abs/1207.0580">dropout</a>).</p>
<p>There are many ways to construct word embeddings but the best approaches are computationally efficient and able to leverage very large text corpora. <a href="http://www.aclweb.org/anthology/D14-1162">GloVe</a> is one such algorithm for constructing embeddings that makes use of the co-occurrence counts of each pair of words within fixed-length windows in the corpus. This results in a matrix whose entries $C_{ij}$ refer to the co-occurrence count of word $i$ with word $j$. Typically, these counts will be weighed down proportionally to the offset between the two words to produce a decay effect. The embeddings are then constructed in similar spirit to the following sum-of-squares minimization:</p>

$$
\min_{\mathbf{w}_1, \mathbf{w}_2, \ \ldots} \bigg\{ \sum_i \sum_j (\mathbf{w}_i^T \mathbf{w}_j - \log C_{ij})^2 \bigg\}.
$$

<p>Here each vector $\mathbf{w}_i$ is an embedding for the corresponding word $i$, and is learned by the algorithm. Since the inner product between vector pairs measures their similarity, the minimization above is adjusting these embeddings to have similarities proportional to their co-occurrence counts. While we could apply the GloVe algorithm directly to our story dataset, we can do much better by utilizing existing word embeddings, trained for instance on the set of all Wikipedia articles. The intuition is that this larger dataset will encode much more meaning and interrelatedness into the word embeddings. GloVe was devised by the Stanford NLP group, and you can download their pre-trained embeddings <a href="https://nlp.stanford.edu/projects/glove/">here</a>.</p>
<h3 id="recurrent-networks">Recurrent Networks</h3>
<p>Recurrent neural networks are an easy choice for this problem because they&rsquo;re meant for sequential data. In theory, they can capture long-range dependencies within sequences (which is highly relevant for our given task), but in practice they fall quite short in their base form, unable to capture dependencies as they get further apart. A solution to this problem is the long short-term memory (LSTM) network, which uses a series of layers and gates constructed to both allow for the long-term channeling and forgetting of information [2]. For this problem, we compared the LSTM&rsquo;s performance to another module, the gated recurrent unit (GRU), which is in essence a simplified version of the LSTM [3]. We found that both LSTMs and GRUs perform on par in terms of test accuracy, and our final choice was the GRU because it arrived to optimal performance much sooner.</p>
<p>Back to textual entailment, we can concatenate all 5 sentences together to form one long sequence of word embeddings, feeding this as input to our GRU network. The network&rsquo;s output is constructed by taking the final state vector of the GRU and applying 5 independent linear transforms, resulting in 5 output vectors (of dimension 5 each) to which we apply softmax activations, so we may interpret each as categorical distributions. In other words, each output vector will dictate which sentence should be placed in its respective position in the narrative. We found this straightforward approach to work well, but acknowledge that it can definitely be extended with useful logical constraints, such as preventing the placement of a sentence in more than one position.</p>
<p>Even with LSTM and GRU networks, long-term dependencies may still pose a problem for longer sequences. Dependencies are unidirectionally captured from past to future, and it can get quite difficult to manage ones that exist between the very first elements in a sequence and the very last. One simple modification to address this problem is to use a bidirectional version of the LSTM or GRU, which is essentially feeding a reversed version of each sequence to the network in addition to the original one. Intuitively, this reduces the distance that information needs to travel between elements in the sequence.</p>
<h3 id="attention">Attention</h3>
<p>We can take this idea of reducing information travel distance even further by introducing soft attention to the recurrent network. The idea behind attention is to create a differentiable mechanism by which each of the 5 output vectors can <strong>attend</strong> to parts of the input sequence that are most relevant to prediction. In a standard recurrent net, the final state is exclusively used to make the output predictions. We can modify this by instead using a weighted sum over all states in the GRU to compute the output probabilities [4]. Importantly, these weights are essentially network parameters, and are learned during training. The figure below taken from [4] visualizes this mechanism for a bidirectional network, where the $h_1, \ldots, h_T$ correspond to state vectors, with the arrows discriminating between forward and reverse sequences, the $\alpha$&rsquo;s are the attention weights, and the output vectors are denoted by $y_{t-1}, y_t, \ldots$ and so forth.</p>
<figure class="center">
    <img src="/img/entailment/attention.png" width="33%"/> <figcaption>
            <h4>Visualization of attention mechanism. Figure taken from [4].</h4>
        </figcaption>
</figure>

<p>For our problem, we make use of a related attention mechanism specifically designed for textual entailment [5]. Please refer to the code <a href="https://github.com/punit-haria/textual_entailment/blob/master/rnn_attention.py">here</a> for a Tensorflow implementation of the mechanism. We found this addition to significantly boost performance on the task.</p>
<h3 id="training-and-hyperparameters">Training and Hyperparameters</h3>
<p>The network described above was trained using the <a href="https://arxiv.org/abs/1412.6980">Adam optimizer</a> with the addition of gradient clipping to improve training stability. Moreover, regularization procedures such as <a href="https://arxiv.org/abs/1207.0580">dropout</a> and an L2 penalty really helped with convergence. We used a random search to tune hyperparameters such as the size of word embeddings, GRU state dimension, dropout probability, learning rate, and batch size. Please have a look <a href="https://github.com/punit-haria/textual_entailment">here</a> for implementation details in Tensorflow.</p>
<h3 id="references">References</h3>
<ol>
<li>Rumelhart, D. E., McClelland, J. L., &amp; PDP Research Group. (1987). Parallel distributed processing (Vol. 1). Cambridge, MA: MIT press.</li>
<li>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.</li>
<li>Cho, K., Van Merriënboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</li>
<li>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</li>
<li>Rocktäschel, T., Grefenstette, E., Hermann, K. M., Kočiský, T., &amp; Blunsom, P. (2015). Reasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664.</li>
</ol>

      
      <div class="related">
</div>
      
    </div>
    
  </div>
</section>



<section class="section">
  <div class="container has-text-centered">
    <p>&copy; <a href="https://github.com/punit-haria">Punit Shah</a> 2017</p>
    
      <p>Powered by <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/ribice/kiss">Kiss</a>.</p>
    
  </div>
</section>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-187493056-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>





</body>
</html>

