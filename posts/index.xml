<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ML is just Stats ;)</title>
    <link>https://punit-haria.github.io/posts/</link>
    <description>Recent content in Posts on ML is just Stats ;)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; &lt;a href=&#34;https://github.com/punit-haria&#34;&gt;Punit Shah&lt;/a&gt; 2017</copyright>
    <lastBuildDate>Fri, 17 Nov 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://punit-haria.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Models</title>
      <link>https://punit-haria.github.io/posts/linear-models/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://punit-haria.github.io/posts/linear-models/</guid>
      <description>Several methods for unsupervised learning have seen ubiquitous use, past and present, across many scientific communities. These include factor analysis, principal component analysis (PCA), Gaussian mixture models, vector quantization, Kalman filters, and hidden Markov models (HMMs). In this post, I outline how these methods can be unified into a single framework, as is shown in the classic Roweis and Ghahramani (1999) paper.
Factor analysis is a method for explaining the variance among observations using a smaller set of unobserved (or latent) factors.</description>
    </item>
    
  </channel>
</rss>
